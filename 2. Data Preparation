# Comvertimg variable types & setting number of decimals: 

# Transform int64 variables to categorical vairables 
df['MPostnr'] = df['MPostnr'].astype('category')
df['MBy'] = df['MBy'].astype('category')
df['AEPostnr'] = df['AEPostnr'].astype('category')
df['AEBy'] = df['AEBy'].astype('category')
df['BPostnr'] = df['BPostnr'].astype('category')
df['BBy'] = df['BBy'].astype('category')
df['B2Postnr'] = df['B2Postnr'].astype('category')
df['B2By'] = df['B2By'].astype('category')
df['B3Postnr'] = df['B3Postnr'].astype('category')
df['B3By'] = df['B3By'].astype('category')

# Transform integer variables to floats 
df['IndexKvartal']=df['IndexKvartal'].astype('float64')
df['IndexAar']=df['IndexAar'].astype('float64')
df['offentligeYdelser']=df['offentligeYdelser'].astype('float64')
df['privateIndtaegter']=df['privateIndtaegter'].astype('float64')

# Transform string/character variables to categorical variables 
df['MBy']=df['MBy'].cat.codes
df['AEBy']=df['AEBy'].cat.codes
df['BBy']=df['BBy'].cat.codes
df['B2By']=df['B2By'].cat.codes
df['B3By']=df['B3By'].cat.codes

# Set all cell values in DF to include only two decimals: 
df=df.round(2)

# Set empty cells ("") to NAN
df=df.replace(r'^\s*$', np.nan, regex=True)

# Variables being dropped due to correlation 
df=df.drop(['id','AEPostnr','BPostnr','B2Postnr','B3Postnr','MPostnr','IndexKvartal','B3By','B3Alder',
            'B2Alder','UDKTypeEkstraBTilskudEnlig','KTypePensKom'],axis=1)
            
            
            
########################### Creating training/test split: ########################################

from sklearn.model_selection import train_test_split


x=df.iloc[:,1:]
y=df.iloc[:,0]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 2)

print('Number of observations and columns in x train', x_train.shape)
print('Number of observations and columns in y train', y_train.shape)
print('Number of observations and columns in x test' , x_test.shape)
print('Number of observations and columns in y test' , y_test.shape)


########################## Balance methods ##########################
# From above it is clear that our distribution is highly skewed - SMOTE:  
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN
from imblearn.under_sampling import RandomUnderSampler


########################## Resampling ##########################
sm=SMOTE(random_state=2)
x_train_smo,y_train_smo=sm.fit_sample(x_train,y_train)

sma=ADASYN(random_state=3)
x_train_ada,y_train_ada=sma.fit_sample(x_train,y_train)

rus=RandomUnderSampler(random_state=4)
x_train_us,y_train_us=rus.fit_sample(x_train,y_train)
x_test_us,y_test_us=rus.fit_sample(x_test,y_test)


########################## Target variable conversion ##########################
y_train_smo=pd.DataFrame(y_train_smo)
y_train_smo =y_train_smo.rename(columns={0: 'Target'})

y_train_ada=pd.DataFrame(y_train_ada)
y_train_ada =y_train_ada.rename(columns={0: 'Target'})

y_train_us=pd.DataFrame(y_train_us)
y_train_us =y_train_us.rename(columns={0: 'Target'})

########################## Distribution plots ##########################
# SMOTE distribution plot  
pd.value_counts(y_train_smo['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')  
y_train_smo['Target'].value_counts()

# ADASYN distribution plot
pd.value_counts(y_train_ada['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency') 
y_train_ada['Target'].value_counts()

# Undersampling distribution plot  
pd.value_counts(y_train_us['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
y_train_us['Target'].value_counts()
