#General settings and data import 
import pandas as pd 
import numpy as np

file_path='/Users/Peter/Downloads/KGML1.csv'
file_path='C:/Users/ander/OneDrive/Skrivebord/KGML1.csv'
df=pd.read_csv(file_path,sep=';',decimal=",")

pd.set_option('display.max_columns',500)
pd.set_option('display.max_rows',500)

#Overview - summary statistics, data types & missing values 
print(df.describe())
print(df.dtypes)
print(df.isnull().sum())

# Comvertimg variable types & setting number of decimals: 

# Transform int64 variables to categorical vairables 
df['MPostnr'] = df['MPostnr'].astype('category')
df['MBy'] = df['MBy'].astype('category')
df['AEPostnr'] = df['AEPostnr'].astype('category')
df['AEBy'] = df['AEBy'].astype('category')
df['BPostnr'] = df['BPostnr'].astype('category')
df['BBy'] = df['BBy'].astype('category')
df['B2Postnr'] = df['B2Postnr'].astype('category')
df['B2By'] = df['B2By'].astype('category')
df['B3Postnr'] = df['B3Postnr'].astype('category')
df['B3By'] = df['B3By'].astype('category')

# Transform integer variables to floats 
df['IndexKvartal']=df['IndexKvartal'].astype('float64')
df['IndexAar']=df['IndexAar'].astype('float64')
df['offentligeYdelser']=df['offentligeYdelser'].astype('float64')
df['privateIndtaegter']=df['privateIndtaegter'].astype('float64')

# Transform string/character variables to categorical variables 
df['MBy']=df['MBy'].cat.codes
df['AEBy']=df['AEBy'].cat.codes
df['BBy']=df['BBy'].cat.codes
df['B2By']=df['B2By'].cat.codes
df['B3By']=df['B3By'].cat.codes

# Set all cell values in DF to include only two decimals: 
df=df.round(2)

# Set empty cells ("") to NAN
df=df.replace(r'^\s*$', np.nan, regex=True)

# Import of packages for plotting: 
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.offline import iplot, init_notebook_mode


# Using plotly + cufflinks in offline mode
import cufflinks
cufflinks.go_offline(connected=True)
init_notebook_mode(connected=True)

# Intercorrelation between variables 
df.corr() # Inspect values 

#Create Correlogram - Heatmap: 
corrs = df.corr() # We plot them visually
figure = ff.create_annotated_heatmap(
    z=corrs.values,
    x=list(corrs.columns),
    y=list(corrs.index),
    annotation_text=corrs.round(2).values,
    showscale=True)
        
figure.iplot()

# Visualizing the target:  

import matplotlib.pyplot as plt
pd.value_counts(df['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
df['Target'].value_counts()

# Creating training/test split: 

from sklearn.model_selection import train_test_split

df=df.drop('id',axis=1)
df=df.drop(['AEPostnr','BPostnr','B2Postnr','B3Postnr','MPostnr'],axis=1)

x=df.iloc[:,1:]
y=df.iloc[:,0]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 2)

print('Number of observations and columns in x train', x_train.shape)
print('Number of observations and columns in y train', y_train.shape)
print('Number of observations and columns in x test' , x_test.shape)
print('Number of observations and columns in y test' , y_test.shape)

# From above it is clear that our distribution is highly skewed - SMOTE:  
from imblearn.over_sampling import SMOTE

sm=SMOTE(random_state=2)
x_train_smo,y_train_smo=sm.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_smo=pd.DataFrame(y_train_smo)
y_train_smo =y_train_smo.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_smo['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_smo['Target'].value_counts()

from imblearn.over_sampling import ADASYN
sm=ADASYN(random_state=3)
x_train_ada,y_train_ada=sm.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_ada=pd.DataFrame(y_train_ada)
y_train_ada =y_train_ada.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_ada['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_ada['Target'].value_counts()

from imblearn.under_sampling import RandomUnderSampler
rus=RandomUnderSampler(random_state=4)
x_train_us,y_train_us=rus.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_us=pd.DataFrame(y_train_us)
y_train_us =y_train_us.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_us['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_us['Target'].value_counts()

# Converting Array to DataFrame
x_train_smo=pd.DataFrame(x_train_smo,columns=[x_test])
x_train_ada=pd.DataFrame(x_train_ada,columns=[x_test])
x_train_us=pd.DataFrame(x_train_us,columns=[x_test])

# Training XgBoost classifiers
from xgboost import XGBClassifier
model_smo = XGBClassifier()
model_smo.fit(x_train_smo, y_train_smo, verbose=False)

model_ada = XGBClassifier()
model_ada.fit(x_train_ada, y_train_ada, verbose=False)

model_us = XGBClassifier()
model_us.fit(x_train_us, y_train_us, verbose=False)

# Prediction 
pred_smo=model_smo.predict(x_test)
pred_ada=model_ada.predict(x_test)
pred_us=model_us.predict(x_test)

# Confusion Matrix for XgBoost model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,pred_smo))
print(confusion_matrix(y_test,pred_ada))
print(confusion_matrix(y_test,pred_us))

# Predictive accuracy for XgBoost model
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, pred_smo))
print(accuracy_score(y_test, pred_ada))
print(accuracy_score(y_test, pred_us))


############### Thursday 18/4 ##################

#Logistic Regression: 
    from sklearn.linear_model import LogisticRegression
    from sklearn import metrics
    
    #Fitting SMO model: 
    log_smo = LogisticRegression()
    log_smo.fit(x_train_smo, y_train_smo)
    
    #Fitting ADA model: 
    log_ada = LogisticRegression()
    log_ada.fit(x_train_ada, y_train_ada)

    #Fitting US model: 
    log_us = LogisticRegression()
    log_us.fit(x_train_us, y_train_us) 
        
    #Prediction on test data:     
    y_pred_smo = log_smo.predict(x_test)
    print('Accuracy of logistic regression classifier - SMO on test set: {:.2f}'.format(log_smo.score(x_test, y_test)))

    y_pred_ada = log_ada.predict(x_test)
    print('Accuracy of logistic regression classifier - ADA on test set: {:.2f}'.format(log_ada.score(x_test, y_test)))

    y_pred_us = log_us.predict(x_test)
    print('Accuracy of logistic regression classifier - US on test set: {:.2f}'.format(log_us.score(x_test, y_test)))
    
    #Confusion matrices: 
    from sklearn.metrics import confusion_matrix
    confusion_matrix_smo = confusion_matrix(y_test, y_pred_smo)
    print(confusion_matrix_smo)

    confusion_matrix_ada = confusion_matrix(y_test, y_pred_ada)
    print(confusion_matrix_ada)

    confusion_matrix_us = confusion_matrix(y_test, y_pred_us)
    print(confusion_matrix_us)

# Support Vector Machines (SVM's): 
   
    from sklearn import svm
    
    #Training models: 
    svc_smo = svm.SVC(gamma='scale')
    svc_smo.fit(x_train_smo, y_train_smo)

    svc_ada = svm.SVC(gamma='scale')
    svc_ada.fit(x_train_ada, y_train_ada)

    svc_us = svm.SVC(gamma='scale')
    svc_us.fit(x_train_us, y_train_us)
    
    #Predictions: 
    svc_pred_smo = svc_smo.predict(x_test)
    print('Accuracy of SVM classifier - SMO on test set: {:.2f}'.format(svc_smo.score(x_test, y_test)))

    svc_pred_ada = svc_ada.predict(x_test)
    print('Accuracy of SVM classifier - ADA on test set: {:.2f}'.format(svc_ada.score(x_test, y_test)))

    svc_pred_us = svc_us.predict(x_test)
    print('Accuracy of SVM classifier - US on test set: {:.2f}'.format(svc_us.score(x_test, y_test)))
    
    #Confusion matrices: 
    from sklearn.metrics import confusion_matrix
    confusion_matrix_smo_svc = confusion_matrix(y_test, svc_pred_smo)
    print(confusion_matrix_smo)

    confusion_matrix_ada_svc = confusion_matrix(y_test, svc_pred_ada)
    print(confusion_matrix_ada)

    confusion_matrix_us_svc = confusion_matrix(y_test, svc_pred_us)
    print(confusion_matrix_us)
    
#Other classifiers: 

# Adaboost: 

    from sklearn.model_selection import cross_val_score
    from sklearn.datasets import load_iris
    from sklearn.ensemble import AdaBoostClassifier
    
    #Training, prediction and calculating accuracy: 
    clf = AdaBoostClassifier(n_estimators=100)   
    scores_smo = cross_val_score(clf, x_train_smo, y_train_smo, cv=5)
    print(scores_smo.mean())
    scores_ada = cross_val_score(clf, x_train_ada, y_train_ada, cv=5)
    print(scores_ada.mean())
    scores_us = cross_val_score(clf, x_train_us, y_train_us, cv=5)
    print(scores_us.mean())
    
# Gradient Boosting Classifier: 
    from sklearn.ensemble import GradientBoostingClassifier
    
    #Training, prediction and calculating accuracy: 
    clf_smo = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train_smo, y_train_smo)
    clf_pred_smo = clf_smo.predict(x_test)
    print(clf_smo.score(x_test, y_test))  

    clf_ada = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train_ada, y_train_ada)
    clf_pred_ada = clf_ada.predict(x_test)
    print(clf_ada.score(x_test, y_test))

    clf_us = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train_us, y_train_us)
    clf_pred_us = clf_us.predict(x_test)
    print(clf_us.score(x_test, y_test))
    
    #Confusion matrices: 
    from sklearn.metrics import confusion_matrix

    confusion_matrix_smo_clf = confusion_matrix(y_test, clf_pred_smo)
    print(confusion_matrix_smo_clf)

    confusion_matrix_ada_clf = confusion_matrix(y_test, clf_pred_ada)
    print(confusion_matrix_ada_clf)

    confusion_matrix_us_clf = confusion_matrix(y_test, clf_pred_us)
    print(confusion_matrix_us_clf)

