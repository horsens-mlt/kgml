#General settings and data import 
import pandas as pd 
import numpy as np

file_path='/Users/Peter/Downloads/KGML1.csv'
file_path='C:/Users/ander/OneDrive/Skrivebord/KGML1.csv'
df=pd.read_csv(file_path,sep=';',decimal=",")

pd.set_option('display.max_columns',500)
pd.set_option('display.max_rows',500)

#Overview - summary statistics, data types & missing values 
print(df.describe())
print(df.dtypes)
print(df.isnull().sum())

# Comvertimg variable types & setting number of decimals: 

# Transform int64 variables to categorical vairables 
df['MPostnr'] = df['MPostnr'].astype('category')
df['MBy'] = df['MBy'].astype('category')
df['AEPostnr'] = df['AEPostnr'].astype('category')
df['AEBy'] = df['AEBy'].astype('category')
df['BPostnr'] = df['BPostnr'].astype('category')
df['BBy'] = df['BBy'].astype('category')
df['B2Postnr'] = df['B2Postnr'].astype('category')
df['B2By'] = df['B2By'].astype('category')
df['B3Postnr'] = df['B3Postnr'].astype('category')
df['B3By'] = df['B3By'].astype('category')

# Transform integer variables to floats 
df['IndexKvartal']=df['IndexKvartal'].astype('float64')
df['IndexAar']=df['IndexAar'].astype('float64')
df['offentligeYdelser']=df['offentligeYdelser'].astype('float64')
df['privateIndtaegter']=df['privateIndtaegter'].astype('float64')

# Transform string/character variables to categorical variables 
df['MBy']=df['MBy'].cat.codes
df['AEBy']=df['AEBy'].cat.codes
df['BBy']=df['BBy'].cat.codes
df['B2By']=df['B2By'].cat.codes
df['B3By']=df['B3By'].cat.codes

# Set all cell values in DF to include only two decimals: 
df=df.round(2)

# Set empty cells ("") to NAN
df=df.replace(r'^\s*$', np.nan, regex=True)

# Import of packages for plotting: 
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly.offline import iplot, init_notebook_mode


# Using plotly + cufflinks in offline mode
import cufflinks
cufflinks.go_offline(connected=True)
init_notebook_mode(connected=True)

# Intercorrelation between variables 
df.corr() # Inspect values 

#Create Correlogram - Heatmap: 
corrs = df.corr() # We plot them visually
figure = ff.create_annotated_heatmap(
    z=corrs.values,
    x=list(corrs.columns),
    y=list(corrs.index),
    annotation_text=corrs.round(2).values,
    showscale=True)
        
figure.iplot()

# Visualizing the target:  

import matplotlib.pyplot as plt
pd.value_counts(df['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
df['Target'].value_counts()

# Creating training/test split: 

from sklearn.model_selection import train_test_split

df=df.drop('id',axis=1)
df=df.drop(['AEPostnr','BPostnr','B2Postnr','B3Postnr','MPostnr'],axis=1)

x=df.iloc[:,1:]
y=df.iloc[:,0]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 2)

print('Number of observations and columns in x train', x_train.shape)
print('Number of observations and columns in y train', y_train.shape)
print('Number of observations and columns in x test' , x_test.shape)
print('Number of observations and columns in y test' , y_test.shape)

# From above it is clear that our distribution is highly skewed - SMOTE:  
from imblearn.over_sampling import SMOTE

sm=SMOTE(random_state=2)
x_train_smo,y_train_smo=sm.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_smo=pd.DataFrame(y_train_smo)
y_train_smo =y_train_smo.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_smo['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_smo['Target'].value_counts()

from imblearn.over_sampling import ADASYN
sm=ADASYN(random_state=3)
x_train_ada,y_train_ada=sm.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_ada=pd.DataFrame(y_train_ada)
y_train_ada =y_train_ada.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_ada['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_ada['Target'].value_counts()

from imblearn.under_sampling import RandomUnderSampler
rus=RandomUnderSampler(random_state=4)
x_train_us,y_train_us=rus.fit_sample(x_train,y_train)

# Convert variable to a dataframe
y_train_us=pd.DataFrame(y_train_us)
y_train_us =y_train_us.rename(columns={0: 'Target'})

# Balanced data visual plot 
pd.value_counts(y_train_us['Target']).plot.bar()
plt.title('Benefit Fraud histogram')
plt.xlabel('Class')
plt.ylabel('Frequency')
    
y_train_us['Target'].value_counts()

# Converting Array to DataFrame
x_train_smo=pd.DataFrame(x_train_smo,columns=[x_test])
x_train_ada=pd.DataFrame(x_train_ada,columns=[x_test])
x_train_us=pd.DataFrame(x_train_us,columns=[x_test])

# Training XgBoost classifiers
from xgboost import XGBClassifier
model_smo = XGBClassifier()
model_smo.fit(x_train_smo, y_train_smo, verbose=False)

model_ada = XGBClassifier()
model_ada.fit(x_train_ada, y_train_ada, verbose=False)

model_us = XGBClassifier()
model_us.fit(x_train_us, y_train_us, verbose=False)

# Prediction 
pred_smo=my_model.predict(x_test)
pred_ada=my_model.predict(x_test)
pred_us=my_model.predict(x_test)

# Confusion Matrix for XgBoost model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,pred_smo))
print(confusion_matrix(y_test,pred_ada))
print(confusion_matrix(y_test,pred_us))

# Predictive accuracy for XgBoost model
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, pred_smo))
print(accuracy_score(y_test, pred_ada))
print(accuracy_score(y_test, pred_us))
